{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c490dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82596eb69ab648579770da520c41cf9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "# or from huggingface_hub import notebook_login # if in a Jupyter/Colab notebook\n",
    "\n",
    "# This will prompt you to enter your token.\n",
    "login()\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b1aa3d-4cdf-4c57-a003-ac75f5a56305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 unique act_tags. Here’s the mapping:\n",
      "    0 → B\n",
      "    1 → D\n",
      "    2 → F\n",
      "    3 → Q\n",
      "    4 → S\n",
      "train → 75067 examples; act_tag dtype = ClassLabel(names=['B', 'D', 'F', 'Q', 'S'], id=None)\n",
      "validation → 16433 examples; act_tag dtype = ClassLabel(names=['B', 'D', 'F', 'Q', 'S'], id=None)\n",
      "test → 16702 examples; act_tag dtype = ClassLabel(names=['B', 'D', 'F', 'Q', 'S'], id=None)\n",
      "\n",
      "Final DatasetDict schema:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change'],\n",
      "        num_rows: 75067\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change'],\n",
      "        num_rows: 16433\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change'],\n",
      "        num_rows: 16702\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cd20c9a6ea4d6f9319f6e34b556c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7d1eef715048bb8dd5ce11c24ec5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/76 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01872d71b80947b49a79a2c83ba2257e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef1e854549f4d31860c1091880ff316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958f14eddcd345819db3904de2b2305b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23b918111d24d93ab15cdfe04401dd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b569100ada754cb9a4b11dfc06e08457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed to https://huggingface.co/datasets/nico8771/mrda_processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
    "# from huggingface_hub import login # Keep if you use programmatic login\n",
    "\n",
    "def create_hf_dataset_from_concatenated_files(\n",
    "    base_path=\".\",\n",
    "    split_files=None,\n",
    "    column_names=None,\n",
    "    delimiter=\"|\"\n",
    "):\n",
    "    if split_files is None:\n",
    "        split_files = {\n",
    "            \"train\": \"train.txt\",\n",
    "            \"validation\": \"val.txt\",\n",
    "            \"test\": \"test.txt\"\n",
    "        }\n",
    "    if column_names is None:\n",
    "        column_names = ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change']\n",
    "\n",
    "    dataset_dict = {}\n",
    "\n",
    "    # Define the features (schema) for the dataset\n",
    "    features = Features({\n",
    "        'caller': Value('string'),\n",
    "        'text': Value('string'),\n",
    "        'act_tag': Value('string'),\n",
    "        'conversation_no': Value('string'), # Explicitly string here\n",
    "        'speaker_change': ClassLabel(num_classes=2, names=['no_change', 'change'])\n",
    "    })\n",
    "\n",
    "    # Explicitly define dtypes for pandas to read all specified columns as strings\n",
    "    # This is the most robust way to prevent misinterpretation\n",
    "    pandas_dtypes = {col_name: str for col_name in column_names}\n",
    "\n",
    "    print(\"Creating Hugging Face Dataset...\")\n",
    "    for split_name, filename in split_files.items():\n",
    "        filepath = os.path.join(base_path, filename)\n",
    "        print(f\"\\nProcessing split: '{split_name}' from file: '{filepath}'\")\n",
    "\n",
    "        if not os.path.exists(filepath) or os.path.getsize(filepath) == 0:\n",
    "            print(f\"Warning: File not found or empty for split '{split_name}': {filepath}. Creating an empty dataset.\")\n",
    "            dataset_dict[split_name] = Dataset.from_dict({}, features=features)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Use the explicit pandas_dtypes dictionary\n",
    "            df = pd.read_csv(\n",
    "                filepath,\n",
    "                delimiter=delimiter,\n",
    "                header=None,\n",
    "                names=column_names,\n",
    "                dtype=pandas_dtypes,  # Apply explicit string type for all columns\n",
    "                keep_default_na=False, # Important to treat empty strings as \"\" not NaN\n",
    "                na_filter=False        # Disables NaN detection based on default NaN values\n",
    "            )\n",
    "\n",
    "            # Debug: Check dtypes after loading and first few values\n",
    "            # print(f\"DEBUG: DataFrame dtypes for {split_name}:\")\n",
    "            # print(df.dtypes)\n",
    "            # print(f\"DEBUG: First 5 'conversation_no' values for {split_name} from DataFrame:\")\n",
    "            # print(df['conversation_no'].head())\n",
    "\n",
    "\n",
    "            # Ensure the 'speaker_change' column is integer for ClassLabel\n",
    "            # This conversion happens AFTER reading it as a string\n",
    "            if 'speaker_change' in df.columns:\n",
    "                # Convert to numeric, coercing errors, then fill NaNs (from coercion or empty strings) with 0, then to int\n",
    "                df['speaker_change'] = pd.to_numeric(df['speaker_change'], errors='coerce').fillna(0).astype(int)\n",
    "            else:\n",
    "                print(f\"Warning: 'speaker_change' column not found in {filepath}. Adding a default column of 0s.\")\n",
    "                df['speaker_change'] = 0 # Assign an integer directly\n",
    "\n",
    "            # Convert pandas DataFrame to Hugging Face Dataset\n",
    "            hf_dataset = Dataset.from_pandas(df, features=features, preserve_index=False)\n",
    "            dataset_dict[split_name] = hf_dataset\n",
    "            print(f\"Successfully created Dataset for split: '{split_name}' with {len(hf_dataset)} examples.\")\n",
    "            if len(hf_dataset) > 0:\n",
    "                print(f\"First example from '{split_name}': {hf_dataset[0]}\")\n",
    "                # Debug: check conversation_no in the HF dataset\n",
    "                # print(f\"DEBUG: 'conversation_no' in first HF example: {hf_dataset[0]['conversation_no']}\")\n",
    "\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: File for split '{split_name}' is empty: {filepath}\")\n",
    "            dataset_dict[split_name] = Dataset.from_dict({}, features=features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filepath} for split '{split_name}': {e}\")\n",
    "            print(f\"Creating an empty dataset for split '{split_name}' due to error.\")\n",
    "            dataset_dict[split_name] = Dataset.from_dict({}, features=features)\n",
    "\n",
    "    final_dataset_dict = DatasetDict(dataset_dict)\n",
    "    print(\"\\n--- Hugging Face DatasetDict created successfully! ---\")\n",
    "    return final_dataset_dict\n",
    "\n",
    "# ... (rest of your __main__ block) ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration (same as before) ---\n",
    "    PROJECT_BASE_PATH = \".\"\n",
    "    SPLIT_FILENAMES = {\n",
    "        \"train\":      \"train.txt\",\n",
    "        \"validation\": \"val.txt\",\n",
    "        \"test\":       \"test.txt\",\n",
    "    }\n",
    "    COLUMN_NAMES_ORDERED = ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change']\n",
    "    FILE_DELIMITER = \"|\"\n",
    "\n",
    "    # 1) Load each split into pandas, same as your function does—but collect all tags\n",
    "    all_tags = set()\n",
    "    df_splits = {}\n",
    "    for split_name, filename in SPLIT_FILENAMES.items():\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(PROJECT_BASE_PATH, filename),\n",
    "            delimiter=FILE_DELIMITER,\n",
    "            header=None,\n",
    "            names=COLUMN_NAMES_ORDERED,\n",
    "            dtype={col: str for col in COLUMN_NAMES_ORDERED},\n",
    "            keep_default_na=False,\n",
    "            na_filter=False,\n",
    "        )\n",
    "        # coerce speaker_change to int as before\n",
    "        df['speaker_change'] = pd.to_numeric(df['speaker_change'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        df_splits[split_name] = df\n",
    "        # only collect from train (or you could union all three)\n",
    "        if split_name == \"train\":\n",
    "            all_tags.update(df['act_tag'].unique())\n",
    "\n",
    "    # 2) Build the ClassLabel feature from sorted tags\n",
    "    unique_tags = sorted(all_tags)\n",
    "    print(f\"Found {len(unique_tags)} unique act_tags. Here’s the mapping:\")\n",
    "    for idx, tag in enumerate(unique_tags):\n",
    "        print(f\"  {idx:3d} → {tag}\")\n",
    "\n",
    "    act_tag_feature = ClassLabel(names=unique_tags)\n",
    "\n",
    "    # 3) Create HuggingFace Datasets with that schema\n",
    "    dataset_dict = {}\n",
    "    for split_name, df in df_splits.items():\n",
    "        features = Features({\n",
    "            'caller':          Value('string'),\n",
    "            'text':            Value('string'),\n",
    "            # now we use our ClassLabel for act_tag:\n",
    "            'act_tag':         act_tag_feature,\n",
    "            'conversation_no': Value('string'),\n",
    "            'speaker_change':  ClassLabel(names=['no_change','change']),\n",
    "        })\n",
    "        ds = Dataset.from_pandas(df, features=features, preserve_index=False)\n",
    "        dataset_dict[split_name] = ds\n",
    "        print(f\"{split_name} → {len(ds)} examples; act_tag dtype = {ds.features['act_tag']}\")\n",
    "\n",
    "    final_dataset = DatasetDict(dataset_dict)\n",
    "    print(\"\\nFinal DatasetDict schema:\")\n",
    "    print(final_dataset)\n",
    "\n",
    "    # 4) Push to the Hub\n",
    "    HUB_REPO_ID = \"nico8771/mrda_processed\"  # ← your repo\n",
    "    final_dataset.push_to_hub(HUB_REPO_ID)\n",
    "    print(f\"Pushed to https://huggingface.co/datasets/{HUB_REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24310f08-927a-40b0-b7ce-8ee96182a7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d751a8-e242-4bf8-8a7f-c631b9adf88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
