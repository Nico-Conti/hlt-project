{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c490dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac1ef1aff1d4ecfada58ca63498ffc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "# or from huggingface_hub import notebook_login # if in a Jupyter/Colab notebook\n",
    "\n",
    "# This will prompt you to enter your token.\n",
    "login()\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b1aa3d-4cdf-4c57-a003-ac75f5a56305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 41 unique act_tags. Here’s the mapping:\n",
      "    0 → %\n",
      "    1 → ^2\n",
      "    2 → ^g\n",
      "    3 → ^h\n",
      "    4 → ^q\n",
      "    5 → aa\n",
      "    6 → aap_am\n",
      "    7 → ad\n",
      "    8 → ar\n",
      "    9 → arp_nd\n",
      "   10 → b\n",
      "   11 → b^m\n",
      "   12 → ba\n",
      "   13 → bd\n",
      "   14 → bf\n",
      "   15 → bh\n",
      "   16 → bk\n",
      "   17 → br\n",
      "   18 → fa\n",
      "   19 → fc\n",
      "   20 → fo_o_fw_\"_by_bc\n",
      "   21 → fp\n",
      "   22 → ft\n",
      "   23 → h\n",
      "   24 → na\n",
      "   25 → ng\n",
      "   26 → nn\n",
      "   27 → no\n",
      "   28 → ny\n",
      "   29 → oo_co_cc\n",
      "   30 → qh\n",
      "   31 → qo\n",
      "   32 → qrr\n",
      "   33 → qw\n",
      "   34 → qw^d\n",
      "   35 → qy\n",
      "   36 → qy^d\n",
      "   37 → sd\n",
      "   38 → sv\n",
      "   39 → t1\n",
      "   40 → t3\n",
      "train → 192386 examples; act_tag dtype = ClassLabel(names=['%', '^2', '^g', '^h', '^q', 'aa', 'aap_am', 'ad', 'ar', 'arp_nd', 'b', 'b^m', 'ba', 'bd', 'bf', 'bh', 'bk', 'br', 'fa', 'fc', 'fo_o_fw_\"_by_bc', 'fp', 'ft', 'h', 'na', 'ng', 'nn', 'no', 'ny', 'oo_co_cc', 'qh', 'qo', 'qrr', 'qw', 'qw^d', 'qy', 'qy^d', 'sd', 'sv', 't1', 't3'], id=None)\n",
      "validation → 3272 examples; act_tag dtype = ClassLabel(names=['%', '^2', '^g', '^h', '^q', 'aa', 'aap_am', 'ad', 'ar', 'arp_nd', 'b', 'b^m', 'ba', 'bd', 'bf', 'bh', 'bk', 'br', 'fa', 'fc', 'fo_o_fw_\"_by_bc', 'fp', 'ft', 'h', 'na', 'ng', 'nn', 'no', 'ny', 'oo_co_cc', 'qh', 'qo', 'qrr', 'qw', 'qw^d', 'qy', 'qy^d', 'sd', 'sv', 't1', 't3'], id=None)\n",
      "test → 4078 examples; act_tag dtype = ClassLabel(names=['%', '^2', '^g', '^h', '^q', 'aa', 'aap_am', 'ad', 'ar', 'arp_nd', 'b', 'b^m', 'ba', 'bd', 'bf', 'bh', 'bk', 'br', 'fa', 'fc', 'fo_o_fw_\"_by_bc', 'fp', 'ft', 'h', 'na', 'ng', 'nn', 'no', 'ny', 'oo_co_cc', 'qh', 'qo', 'qrr', 'qw', 'qw^d', 'qy', 'qy^d', 'sd', 'sv', 't1', 't3'], id=None)\n",
      "\n",
      "Final DatasetDict schema:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change'],\n",
      "        num_rows: 192386\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change'],\n",
      "        num_rows: 3272\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change'],\n",
      "        num_rows: 4078\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459bdbbf18f94bc18c074b88331c5a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78b6d40f3e764f3dbed19b1b44fce4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/193 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9001f4178af46e8bd6d909838c98590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e635e80fe04e45b9c2881b6fb677d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8bcef1e42e4f5898853423d3902be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553141df6dd748368f1dc357372f04ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushed to https://huggingface.co/datasets/nico8771/swda_processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Features, Value, ClassLabel\n",
    "# from huggingface_hub import login # Keep if you use programmatic login\n",
    "\n",
    "def create_hf_dataset_from_concatenated_files(\n",
    "    base_path=\".\",\n",
    "    split_files=None,\n",
    "    column_names=None,\n",
    "    delimiter=\"|\"\n",
    "):\n",
    "    if split_files is None:\n",
    "        split_files = {\n",
    "            \"train\": \"train.txt\",\n",
    "            \"validation\": \"val.txt\",\n",
    "            \"test\": \"test.txt\"\n",
    "        }\n",
    "    if column_names is None:\n",
    "        column_names = ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change']\n",
    "\n",
    "    dataset_dict = {}\n",
    "\n",
    "    # Define the features (schema) for the dataset\n",
    "    features = Features({\n",
    "        'caller': Value('string'),\n",
    "        'text': Value('string'),\n",
    "        'act_tag': Value('string'),\n",
    "        'conversation_no': Value('string'), # Explicitly string here\n",
    "        'speaker_change': ClassLabel(num_classes=2, names=['no_change', 'change'])\n",
    "    })\n",
    "\n",
    "    # Explicitly define dtypes for pandas to read all specified columns as strings\n",
    "    # This is the most robust way to prevent misinterpretation\n",
    "    pandas_dtypes = {col_name: str for col_name in column_names}\n",
    "\n",
    "    print(\"Creating Hugging Face Dataset...\")\n",
    "    for split_name, filename in split_files.items():\n",
    "        filepath = os.path.join(base_path, filename)\n",
    "        print(f\"\\nProcessing split: '{split_name}' from file: '{filepath}'\")\n",
    "\n",
    "        if not os.path.exists(filepath) or os.path.getsize(filepath) == 0:\n",
    "            print(f\"Warning: File not found or empty for split '{split_name}': {filepath}. Creating an empty dataset.\")\n",
    "            dataset_dict[split_name] = Dataset.from_dict({}, features=features)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Use the explicit pandas_dtypes dictionary\n",
    "            df = pd.read_csv(\n",
    "                filepath,\n",
    "                delimiter=delimiter,\n",
    "                header=None,\n",
    "                names=column_names,\n",
    "                dtype=pandas_dtypes,  # Apply explicit string type for all columns\n",
    "                keep_default_na=False, # Important to treat empty strings as \"\" not NaN\n",
    "                na_filter=False        # Disables NaN detection based on default NaN values\n",
    "            )\n",
    "\n",
    "            \n",
    "            # ←─── DROP IN YOUR ZERO/ONE SNIFFER HERE ────────────────────\n",
    "            for special_tag in (\"0\", \"1\"):\n",
    "                hits = df[df[\"act_tag\"] == special_tag]\n",
    "                if not hits.empty:\n",
    "                    print(f\"\\nFound {len(hits)} utterance(s) with act_tag = {special_tag} in '{split_name}':\")\n",
    "                    for _, row in hits.iterrows():\n",
    "                        print(f\"  – Caller: {row['caller']!r}; Text: {row['text']!r}\")\n",
    "            # ─────────────────────────────────────────────────────────────\n",
    "            # Debug: Check dtypes after loading and first few values\n",
    "            # print(f\"DEBUG: DataFrame dtypes for {split_name}:\")\n",
    "            # print(df.dtypes)\n",
    "            # print(f\"DEBUG: First 5 'conversation_no' values for {split_name} from DataFrame:\")\n",
    "            # print(df['conversation_no'].head())\n",
    "\n",
    "\n",
    "            # Ensure the 'speaker_change' column is integer for ClassLabel\n",
    "            # This conversion happens AFTER reading it as a string\n",
    "            if 'speaker_change' in df.columns:\n",
    "                # Convert to numeric, coercing errors, then fill NaNs (from coercion or empty strings) with 0, then to int\n",
    "                df['speaker_change'] = pd.to_numeric(df['speaker_change'], errors='coerce').fillna(0).astype(int)\n",
    "            else:\n",
    "                print(f\"Warning: 'speaker_change' column not found in {filepath}. Adding a default column of 0s.\")\n",
    "                df['speaker_change'] = 0 # Assign an integer directly\n",
    "\n",
    "            # Convert pandas DataFrame to Hugging Face Dataset\n",
    "            hf_dataset = Dataset.from_pandas(df, features=features, preserve_index=False)\n",
    "            dataset_dict[split_name] = hf_dataset\n",
    "            print(f\"Successfully created Dataset for split: '{split_name}' with {len(hf_dataset)} examples.\")\n",
    "            if len(hf_dataset) > 0:\n",
    "                print(f\"First example from '{split_name}': {hf_dataset[0]}\")\n",
    "                # Debug: check conversation_no in the HF dataset\n",
    "                # print(f\"DEBUG: 'conversation_no' in first HF example: {hf_dataset[0]['conversation_no']}\")\n",
    "\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: File for split '{split_name}' is empty: {filepath}\")\n",
    "            dataset_dict[split_name] = Dataset.from_dict({}, features=features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filepath} for split '{split_name}': {e}\")\n",
    "            print(f\"Creating an empty dataset for split '{split_name}' due to error.\")\n",
    "            dataset_dict[split_name] = Dataset.from_dict({}, features=features)\n",
    "\n",
    "    final_dataset_dict = DatasetDict(dataset_dict)\n",
    "    print(\"\\n--- Hugging Face DatasetDict created successfully! ---\")\n",
    "    return final_dataset_dict\n",
    "\n",
    "# ... (rest of your __main__ block) ...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration (same as before) ---\n",
    "    PROJECT_BASE_PATH = \".\"\n",
    "    SPLIT_FILENAMES = {\n",
    "        \"train\":      \"train.txt\",\n",
    "        \"validation\": \"val.txt\",\n",
    "        \"test\":       \"test.txt\",\n",
    "    }\n",
    "    COLUMN_NAMES_ORDERED = ['caller', 'text', 'act_tag', 'conversation_no', 'speaker_change']\n",
    "    FILE_DELIMITER = \"|\"\n",
    "\n",
    "    # 1) Load each split into pandas, same as your function does—but collect all tags\n",
    "    all_tags = set()\n",
    "    df_splits = {}\n",
    "    for split_name, filename in SPLIT_FILENAMES.items():\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(PROJECT_BASE_PATH, filename),\n",
    "            delimiter=FILE_DELIMITER,\n",
    "            header=None,\n",
    "            names=COLUMN_NAMES_ORDERED,\n",
    "            dtype={col: str for col in COLUMN_NAMES_ORDERED},\n",
    "            keep_default_na=False,\n",
    "            na_filter=False,\n",
    "        )\n",
    "        # coerce speaker_change to int as before\n",
    "        df['speaker_change'] = pd.to_numeric(df['speaker_change'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        df_splits[split_name] = df\n",
    "        # only collect from train (or you could union all three)\n",
    "        if split_name == \"train\":\n",
    "            all_tags.update(df['act_tag'].unique())\n",
    "    \n",
    "    # 2) Build the ClassLabel feature from sorted tags\n",
    "    unique_tags = sorted(all_tags)\n",
    "    print(f\"Found {len(unique_tags)} unique act_tags. Here’s the mapping:\")\n",
    "    for idx, tag in enumerate(unique_tags):\n",
    "        print(f\"  {idx:3d} → {tag}\")\n",
    "\n",
    "    act_tag_feature = ClassLabel(names=unique_tags)\n",
    "\n",
    "    # 3) Create HuggingFace Datasets with that schema\n",
    "    dataset_dict = {}\n",
    "    for split_name, df in df_splits.items():\n",
    "        features = Features({\n",
    "            'caller':          Value('string'),\n",
    "            'text':            Value('string'),\n",
    "            # now we use our ClassLabel for act_tag:\n",
    "            'act_tag':         act_tag_feature,\n",
    "            'conversation_no': Value('string'),\n",
    "            'speaker_change':  ClassLabel(names=['no_change','change']),\n",
    "        })\n",
    "        ds = Dataset.from_pandas(df, features=features, preserve_index=False)\n",
    "        dataset_dict[split_name] = ds\n",
    "        print(f\"{split_name} → {len(ds)} examples; act_tag dtype = {ds.features['act_tag']}\")\n",
    "\n",
    "    final_dataset = DatasetDict(dataset_dict)\n",
    "    print(\"\\nFinal DatasetDict schema:\")\n",
    "    print(final_dataset)\n",
    "\n",
    "    # 4) Push to the Hub\n",
    "    HUB_REPO_ID = \"nico8771/swda_processed\"  # ← your repo\n",
    "    final_dataset.push_to_hub(HUB_REPO_ID)\n",
    "    print(f\"Pushed to https://huggingface.co/datasets/{HUB_REPO_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24310f08-927a-40b0-b7ce-8ee96182a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'caller': 'A', 'text': 'Okay.', 'act_tag': 22, 'conversation_no': '2005', 'speaker_change': 0}\n",
      "{'caller': 'A', 'text': 'Uh, first, um, I need to know, uh, how do you feel about, uh, about sending, uh, an elderly, uh, family member to a nursing home?', 'act_tag': 33, 'conversation_no': '2005', 'speaker_change': 0}\n",
      "{'caller': 'B', 'text': \"Well, of course, it's, you know, it's one of the last few things in the world you'd ever want to do, you know. Unless it's just, you know, really, you know, and, uh, for their, uh, you know, for their own good.\", 'act_tag': 40, 'conversation_no': '2005', 'speaker_change': 1}\n",
      "{'caller': 'A', 'text': 'Yes.', 'act_tag': 7, 'conversation_no': '2005', 'speaker_change': 1}\n",
      "{'caller': 'A', 'text': 'Yeah.', 'act_tag': 7, 'conversation_no': '2005', 'speaker_change': 0}\n",
      "{'caller': 'B', 'text': \"I'd be very very careful and, uh, you know, checking them out.\", 'act_tag': 39, 'conversation_no': '2005', 'speaker_change': 1}\n",
      "{'caller': 'B', 'text': 'Uh, our,', 'act_tag': 0, 'conversation_no': '2005', 'speaker_change': 0}\n",
      "{'caller': 'B', 'text': 'had place my mother in a nursing home.', 'act_tag': 39, 'conversation_no': '2005', 'speaker_change': 0}\n",
      "{'caller': 'B', 'text': 'She had a rather massive stroke about, uh, about uh, eight months ago I guess.', 'act_tag': 39, 'conversation_no': '2005', 'speaker_change': 0}\n",
      "{'caller': 'A', 'text': 'Uh-huh.', 'act_tag': 12, 'conversation_no': '2005', 'speaker_change': 1}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(final_dataset['train'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d751a8-e242-4bf8-8a7f-c631b9adf88a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
